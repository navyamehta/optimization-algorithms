{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> This notebook presents a greedy heuristic algorithm approach to the Google HashCode Optimization Problem, more details for which can be found at https://www.kaggle.com/c/hashcode-photo-slideshow/data. Within this approach, a sorted dictionary of double-ended queues is created to calculate \"interest metric\" values between each pair of images. The highest-value pairs are continuously appended to the string until all elements have been added. After individual horizontal and vertical images are added, a meta-level optimization is performed to club vertical images together. While this may possibly miss global minima as double-vertical slides are not considered in the initial ordering, the split reduces the computational order of magnitude. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gc\n",
    "import psutil\n",
    "import pickle\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import collections\n",
    "import h5py\n",
    "import gc\n",
    "from sortedcontainers import SortedDict, SortedList\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d_pet_pictures.txt', 'rb') as f:\n",
    "    data = f.readlines()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create Greedy Algorithm Helper Functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tagdict(data):\n",
    "    hdict, vdict = {}, {}\n",
    "    for i in range(len(data)):\n",
    "        row = np.vectorize(lambda s: s.decode('utf-8'))(data[i].split())\n",
    "        if row[0]==\"H\": hdict[str(i)] = set(row[1:])\n",
    "        else: vdict[str(i)] = set(row[1:])\n",
    "    return hdict, vdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES INVARIANT\n",
    "#Needs hdict and vdict, dictionaries of image tag sets\n",
    "def calculate_interest(name1, name2):\n",
    "    #Invariant: names are of the form (H/V):K where H/V indicates the dictionary in question and K is the key\n",
    "    #The hdict and vdict dictionaries contain sets\n",
    "    namedisamb = (lambda s: hdict[s.split(\":\")[1]] if s.split(\":\")[0]==\"H\" else \n",
    "                  vdict[s.split(\":\")[1]])\n",
    "    tag1, tag2 = namedisamb(name1), namedisamb(name2)\n",
    "    return min(len(tag1&tag2), len(tag1-tag2), len(tag2-tag1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate interest master list for subset of arrays\n",
    "#Memory limitations prohibit a 90000x90000 master matrix\n",
    "def generate_interest_dict(keys, interestfunc=calculate_interest):\n",
    "    start = time.time()\n",
    "    checkpoints = np.arange(0, keys.shape[0], keys.shape[0]/5)[1:]\n",
    "    interestdict = SortedDict()\n",
    "    for i in range(keys.shape[0]):\n",
    "        for j in range(i+1, keys.shape[0]):\n",
    "            value = interestfunc(keys[i], keys[j])\n",
    "            if value==0: continue\n",
    "            if interestdict.get(value, False)==False:\n",
    "                interestdict[value] = collections.deque([(i,j)])\n",
    "            else:\n",
    "                interestdict[value].append((i,j))\n",
    "        if (i+1) in checkpoints: \n",
    "            print(\"Calculation Checkpoint of \"+str(i+1)+\":\"+str(time.time()-start))\n",
    "    print(time.time()-start)\n",
    "    return interestdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS:\n",
    "#interestdict: a sorted dictionaries of deques containing interest values\n",
    "#lookupdict: a dictionary of occurences\n",
    "#keys: list of names of keys\n",
    "def getmaxfn(interestdict, lookupdict, keys):\n",
    "    curcount1, curcount2 = 2, 2\n",
    "    while (curcount1==2) or (curcount2==2):\n",
    "        if len(interestdict)==0: return None, False\n",
    "        curval = interestdict.keys()[-1]\n",
    "        curmax = interestdict[curval].pop()\n",
    "        curcount1, curcount2 = lookupdict.get(keys[curmax[0]],0), lookupdict.get(keys[curmax[1]],0)\n",
    "        if not interestdict[curval]:\n",
    "            del interestdict[curval]\n",
    "    lookupdict[keys[curmax[0]]] = curcount1+1\n",
    "    lookupdict[keys[curmax[1]]] = curcount2+1\n",
    "    return (keys[curmax[0]], keys[curmax[1]]), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collapse Deque\n",
    "#PARAMETERS:\n",
    "#pair: size-2 tuple containing the new added transition\n",
    "#curarrang: a deque of deques containing current valid transition sequences\n",
    "#lookupdict: a dictionary recording occurences\n",
    "def deque_update(pair, curarrang, lookupdict):\n",
    "    #Resolve Merge of Two Elements\n",
    "    def resolve_double(first, second):\n",
    "        #INVARIANT: The first and second will have an overlapping value as the first was updated but not second\n",
    "        if curarrang[first][0] in [curarrang[second][0], curarrang[second][-1]]:\n",
    "            curarrang[first].reverse()\n",
    "        if curarrang[first][-1]==curarrang[second][0]:\n",
    "            curarrang[second].popleft()\n",
    "        elif curarrang[first][-1]==curarrang[second][-1]:\n",
    "            curarrang[second].reverse()\n",
    "            curarrang[second].popleft()\n",
    "        curarrang[first]+=curarrang[second]\n",
    "        del curarrang[second]\n",
    "    match, lastm = False, 0\n",
    "    i=0\n",
    "    while i<len(curarrang):\n",
    "        if curarrang[i][0]==pair[0]:\n",
    "            #Prevent circular deques\n",
    "            if curarrang[i][-1]==pair[1]: \n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].appendleft(pair[1])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        elif curarrang[i][0]==pair[1]:\n",
    "            if curarrang[i][-1]==pair[0]: \n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].appendleft(pair[0])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        elif curarrang[i][-1]==pair[0]:\n",
    "            if curarrang[i][0]==pair[1]: \n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].append(pair[1])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        elif curarrang[i][-1]==pair[1]:\n",
    "            if curarrang[i][-1]==pair[0]:\n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].append(pair[0])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        else:\n",
    "            i+=1\n",
    "    if match: return\n",
    "    curarrang.append(collections.deque(pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Run Single-Image Greedy Heuristic (Layer 1)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "nsplits = 4\n",
    "hdict, vdict = generate_tagdict(data)\n",
    "hkeys = np.vectorize(lambda s: \"H:\"+str(s))(np.array(list(hdict.keys())))\n",
    "vkeys = np.vectorize(lambda s: \"V:\"+str(s))(np.array(list(vdict.keys())))\n",
    "totalkeys = np.append(hkeys, vkeys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(totalkeys.shape[0])\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(indices)\n",
    "indices = np.split(indices, nsplits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_run(keys, interestfunc=calculate_interest):\n",
    "    interestdict = generate_interest_dict(keys, interestfunc)\n",
    "    lookupdict = {}\n",
    "    curarrang = collections.deque()\n",
    "    while (len(curarrang)==0) or (len(curarrang[0])<len(keys)):\n",
    "        maxval, valid = getmaxfn(interestdict, lookupdict, keys)\n",
    "        #In case all positive interest metrics have been exhausted\n",
    "        if not valid: break\n",
    "        deque_update(maxval, curarrang, lookupdict)\n",
    "    finalarrang = collections.deque()\n",
    "    for arrang in curarrang:\n",
    "        finalarrang+=arrang\n",
    "    del curarrang\n",
    "    remimg = set(keys)-set(finalarrang)\n",
    "    for img in remimg:\n",
    "        finalarrang.append(img)\n",
    "    return finalarrang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Checkpoint of 4500:424.9527807235718\n",
      "Calculation Checkpoint of 9000:822.3509747982025\n",
      "Calculation Checkpoint of 13500:1063.8601069450378\n",
      "Calculation Checkpoint of 18000:1207.6297268867493\n",
      "1255.2410807609558\n",
      "Mask Optimized in 2244.443123102188 seconds\n",
      "Calculation Checkpoint of 4500:444.54129004478455\n",
      "Calculation Checkpoint of 9000:794.7886159420013\n",
      "Calculation Checkpoint of 13500:1043.4002268314362\n",
      "Calculation Checkpoint of 18000:1190.953207731247\n",
      "1237.5998117923737\n",
      "Mask Optimized in 2285.985775947571 seconds\n",
      "Calculation Checkpoint of 4500:464.16116309165955\n",
      "Calculation Checkpoint of 9000:815.0599539279938\n",
      "Calculation Checkpoint of 13500:1061.4540979862213\n",
      "Calculation Checkpoint of 18000:1215.1491250991821\n",
      "1261.6071569919586\n",
      "Mask Optimized in 2252.183938741684 seconds\n",
      "Calculation Checkpoint of 4500:421.3487648963928\n",
      "Calculation Checkpoint of 9000:752.308002948761\n",
      "Calculation Checkpoint of 13500:993.1060469150543\n",
      "Calculation Checkpoint of 18000:1132.5027568340302\n",
      "1177.8050417900085\n",
      "Mask Optimized in 2160.84285902977 seconds\n"
     ]
    }
   ],
   "source": [
    "finalresults = collections.deque()\n",
    "for mask in indices:\n",
    "    start = time.time()\n",
    "    arrang = individual_run(totalkeys[mask])\n",
    "    gc.collect()\n",
    "    finalresults.append(arrang)\n",
    "    print(\"Mask Optimized in \"+str(time.time()-start)+\" seconds\")\n",
    "pickle.dump(finalresults, open(\"./layer1arrang.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generate Helper Functions for Replacement of Double-Vertical Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INVARIANT: Every element of arrang is a 1-element or 2-element tuple\n",
    "def interest_change(index1, index2, arrang):\n",
    "    def important_index(index, arrang):\n",
    "        if (index==0): vals = [arrang[1]]\n",
    "        elif (index==(len(arrang)-1)): vals = [arrang[index-1]]\n",
    "        else: vals = [arrang[index-1],arrang[index+1]]\n",
    "        return vals\n",
    "    def total_value(images, tup):\n",
    "        nameret = (lambda s: hdict[s.split(\":\")[1]] if s.split(\":\")[0]==\"H\" else \n",
    "                  vdict[s.split(\":\")[1]])\n",
    "        interest = 0\n",
    "        main = nameret(tup[0]) if len(tup)==1 else nameret(tup[0]).union(nameret(tup[1]))\n",
    "        for pair in images:\n",
    "            curset = nameret(pair[0]) if len(pair)==1 else nameret(pair[0]).union(nameret(pair[1]))\n",
    "            interest+=min(len(curset&main), len(curset-main), len(main-curset))\n",
    "        return interest\n",
    "    desvals, sourcevals = important_index(index1, arrang), important_index(index2, arrang)\n",
    "    if index1==index2-1: sourcevals = sourcevals[1:]\n",
    "    elif index1==index2+1: sourcevals = sourcevals[:-1]\n",
    "    preinterest=total_value(desvals, arrang[index1]) + total_value(sourcevals, arrang[index2])\n",
    "    if index1==index2-1:\n",
    "        desvals=desvals[:-1]+sourcevals\n",
    "        sourcevals = []\n",
    "    elif index1==index2+1:\n",
    "        desvals=desvals[1:]+sourcevals\n",
    "        sourcevals = []\n",
    "    postinterest=total_value(desvals, (arrang[index1][0], arrang[index2][0]))\n",
    "    if len(sourcevals)>1:\n",
    "        postinterest+=total_value(sourcevals[1:], sourcevals[0])\n",
    "    return postinterest-preinterest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_swap(vertindices, mainindex, arrang):\n",
    "    bestindex, bestinterest, firstdes = -1, None, True\n",
    "    for index in vertindices:\n",
    "        if index==mainindex: continue\n",
    "        value1 = interest_change(mainindex, index, arrang)\n",
    "        value2 = interest_change(index, mainindex, arrang)\n",
    "        if (not bestinterest) or (value1>bestinterest):\n",
    "            bestindex = index\n",
    "            bestinterest = value1\n",
    "            firstdes = True\n",
    "        if (not bestinterest) or (value2>bestinterest):\n",
    "            bestindex = index\n",
    "            bestinterest = value2\n",
    "            firstdes = False\n",
    "    return bestindex, bestinterest, firstdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#While we used a faster one-element interest calculation in Layer 1 to optimize speed, we need tuple support here\n",
    "def interest(tuple1, tuple2):\n",
    "    nameret = (lambda s: hdict[s.split(\":\")[1]] if s.split(\":\")[0]==\"H\" else \n",
    "                  vdict[s.split(\":\")[1]])\n",
    "    tupleset = (lambda s: nameret(s[0]) if len(s)==1 else nameret(s[0]).union(nameret(s[1])))\n",
    "    set1, set2 = tupleset(tuple1), tupleset(tuple2)\n",
    "    return min(len(set1&set2), len(set1-set2), len(set2-set1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Group Double-Vertical Images to Enhance Interest (Layer 2) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresults = pickle.load(open(\"./layer1arrang.pkl\", \"rb\"))\n",
    "vertindices = {i:np.where(np.vectorize(lambda s: 'V' in s)(np.array(finalresults[i])))[0]\n",
    "               for i in range(len(finalresults))}\n",
    "#Uniform data structure (to tuple) for efficient functions\n",
    "for arr in finalresults:\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = (arr[i],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad Tolerance is maximum loss on grouping you are willing to endure. Greater losses can potentially be mitigated\n",
    "#On full 4-level grouping later\n",
    "def grouping_run(arrang, indexset, BAD_TOLERANCE=-4):\n",
    "    i=0\n",
    "    while i<len(arrang):\n",
    "        if len(indexset)<2: return\n",
    "        if (len(arrang[i])==1) and (\"V\" in arrang[i][0]):\n",
    "            bestindex, bestinterest, firstdes = best_swap(indexset, i, arrang)\n",
    "            if not (bestinterest<BAD_TOLERANCE):\n",
    "                indexset = indexset[(indexset!=i)&(indexset!=bestindex)]\n",
    "                if firstdes:\n",
    "                    arrang[i] = (arrang[i][0], arrang[bestindex][0])\n",
    "                    del arrang[bestindex]\n",
    "                    if bestindex>i: i+=1\n",
    "                    indexset[indexset>bestindex]-=1\n",
    "                else:\n",
    "                    arrang[bestindex] = (arrang[i][0], arrang[bestindex][0])\n",
    "                    del arrang[i]\n",
    "                    indexset[indexset>i]-=1\n",
    "            else:\n",
    "                indexset = indexset[(indexset!=i)] #Needs to wait later grouping\n",
    "                i+=1\n",
    "        else:\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical Combinations Optimized for Mask in 3929.275274038315 seconds\n",
      "Vertical Combinations Optimized for Mask in 3783.025017976761 seconds\n",
      "Vertical Combinations Optimized for Mask in 3899.140506029129 seconds\n",
      "Vertical Combinations Optimized for Mask in 3932.5125439167023 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(nsplits):\n",
    "    start = time.time()\n",
    "    grouping_run(finalresults[i], vertindices[i])\n",
    "    gc.collect()\n",
    "    print(\"Vertical Combinations Optimized for Mask in \"+str(time.time()-start)+\" seconds\")\n",
    "pickle.dump(finalresults, open(\"./layer2arrang.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Optimize ordering of the four sub-sequences\n",
    "finalresults = pickle.load(open(\"./layer2arrang.pkl\", \"rb\"))\n",
    "orders = [p for p in permutations(np.arange(nsplits))]\n",
    "maxgain, maxorder = None, None\n",
    "for order in orders:\n",
    "    gain = 0\n",
    "    for pos in range(1, len(order)):\n",
    "        gain+=interest(finalresults[order[pos-1]][-1], finalresults[order[pos]][0])\n",
    "    if (not maxgain) or (gain>maxgain):\n",
    "        maxgain = gain\n",
    "        maxorder = order\n",
    "arrang = collections.deque()\n",
    "for pos in maxorder:\n",
    "    arrang+=finalresults[pos]\n",
    "del finalresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 19min 16s, sys: 10.8 s, total: 1h 19min 27s\n",
      "Wall time: 1h 55min 16s\n"
     ]
    }
   ],
   "source": [
    "finalindexset = np.where(np.vectorize(lambda s: (len(s)==1) and (s[0][0]==\"V\"))(arrang))[0]\n",
    "gc.collect()\n",
    "%time grouping_run(arrang, finalindexset, BAD_TOLERANCE=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finalize and Write Submission File </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Incorrect Vertical Images and 0 Incorrect Horizontal Images\n",
      "All Unique Entries\n"
     ]
    }
   ],
   "source": [
    "#Sanity Checks on Status of Arrangement\n",
    "#CHECK 1: All Vertical Images Occur in Pairs and Horizontals Occur Individually\n",
    "wrongvert = sum([(len(r)==1) and (r[0][0]==\"V\") for r in arrang])\n",
    "wronghorz = sum([(len(r)==2) and (r[0][0]==\"H\") for r in arrang])\n",
    "print(str(wrongvert)+\" Incorrect Vertical Images and \"+str(wronghorz)+\" Incorrect Horizontal Images\")\n",
    "#CHECK 2: All Images are Unique (i.e. no repetitions)\n",
    "def repetition_check(arrang):\n",
    "    checkdict = {}\n",
    "    for pair in arrang:\n",
    "        for elem in pair:\n",
    "            checkdict[elem] = checkdict.get(elem,0)+1\n",
    "            if checkdict[elem]!=1:\n",
    "                return \"Duplicate Encountered\"\n",
    "    return \"All Unique Entries\"\n",
    "print(repetition_check(arrang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Interest Metric Across Sequence: 398411\n"
     ]
    }
   ],
   "source": [
    "#Calculation of Final Interest Metric of Produced Output\n",
    "val = 0\n",
    "for i in range(len(arrang)-1):\n",
    "    val+=interest(arrang[i], arrang[i+1])\n",
    "print(\"Final Interest Metric Across Sequence: \"+str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Output File in Desired Format\n",
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    finallength = str(len(arrang))+\"\\n\"\n",
    "    f.write(finallength)\n",
    "    for slide in arrang:\n",
    "        content = \" \".join([s.split(\":\")[1] for s in slide])+\"\\n\"\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ADDENDUM: Grouping Verticals Pre-Arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "hdict, vdict = generate_tagdict(data)\n",
    "hkeys = np.vectorize(lambda s: \"H:\"+str(s))(np.array(list(hdict.keys())))\n",
    "vkeys = np.vectorize(lambda s: \"V:\"+str(s))(np.array(list(vdict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group a vertical with a \"minimum\"-scoring interest image, preferably zero, and exit\n",
    "def interest_grouper(vimage, imglist):\n",
    "    minscore, minimg = np.nan, None\n",
    "    for img in imglist:\n",
    "        if img==vimage: continue\n",
    "        score = calculate_interest(vimage, img)\n",
    "        if np.isnan(minscore) or (score<minscore):\n",
    "            minscore = score\n",
    "            minimg = img\n",
    "        if minscore==0: return minimg\n",
    "    return minimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical Groupings Exited in 17.25269389152527 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vgroups = collections.deque()\n",
    "i=len(vkeys)-1\n",
    "while i>=0:\n",
    "    match = interest_grouper(vkeys[i], vkeys)\n",
    "    vgroups.append((vkeys[i], match))\n",
    "    vkeys = vkeys[(vkeys!=vkeys[i])&(vkeys!=match)]\n",
    "    i-=2\n",
    "print(\"Vertical Groupings Exited in \"+str(time.time()-start)+\" seconds\")\n",
    "totalkeys = np.array(collections.deque([(s,) for s in hkeys])+vgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsplits = 3\n",
    "indices = np.arange(totalkeys.shape[0])\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(indices)\n",
    "indices = np.array(np.split(indices, nsplits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Checkpoint of 4000:552.3254058361053\n",
      "Calculation Checkpoint of 8000:1025.4317727088928\n",
      "Calculation Checkpoint of 12000:1507.420224905014\n",
      "Calculation Checkpoint of 16000:1693.5363488197327\n",
      "1753.7792398929596\n",
      "Single Mask Optimized in 2672.425518989563 seconds\n",
      "Calculation Checkpoint of 4000:613.1249301433563\n",
      "Calculation Checkpoint of 8000:1070.00234913826\n",
      "Calculation Checkpoint of 12000:1549.8545899391174\n",
      "Calculation Checkpoint of 16000:1737.8045172691345\n",
      "2231.3519072532654\n",
      "Single Mask Optimized in 3209.968752861023 seconds\n",
      "Calculation Checkpoint of 4000:568.7762370109558\n",
      "Calculation Checkpoint of 8000:1142.0105538368225\n",
      "Calculation Checkpoint of 12000:1453.0140118598938\n",
      "Calculation Checkpoint of 16000:1960.3683190345764\n",
      "2022.3335180282593\n",
      "Single Mask Optimized in 2999.7830488681793 seconds\n"
     ]
    }
   ],
   "source": [
    "finalresults = collections.deque()\n",
    "for i in range(nsplits):\n",
    "    start = time.time()\n",
    "    arrang = individual_run(totalkeys[indices[i]], interestfunc=interest)\n",
    "    gc.collect()\n",
    "    finalresults+=arrang\n",
    "    print(\"Single Mask Optimized in \"+str(time.time()-start)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Interest Metric Across Sequence: 475720\n"
     ]
    }
   ],
   "source": [
    "#Calculation of Final Interest Metric of Produced Output\n",
    "val = 0\n",
    "for i in range(len(finalresults)-1):\n",
    "    val+=interest(finalresults[i], finalresults[i+1])\n",
    "print(\"Final Interest Metric Across Sequence: \"+str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Output File in Desired Format\n",
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    finallength = str(len(finalresults))+\"\\n\"\n",
    "    f.write(finallength)\n",
    "    for slide in finalresults:\n",
    "        content = \" \".join([s.split(\":\")[1] for s in slide])+\"\\n\"\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
