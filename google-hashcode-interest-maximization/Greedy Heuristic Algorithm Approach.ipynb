{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> This notebook presents a greedy heuristic algorithm approach to the Google HashCode Optimization Problem, more details for which can be found at https://www.kaggle.com/c/hashcode-photo-slideshow/data. Within this approach, a sorted dictionary of double-ended queues is created to calculate \"interest metric\" values between each pair of images. The highest-value pairs are continuously appended to the string until all elements have been added. After individual horizontal and vertical images are added, a meta-level optimization is performed to club vertical images together. While this may possibly miss global minima as double-vertical slides are not considered in the initial ordering, the split reduces the computational order of magnitude. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gc\n",
    "import psutil\n",
    "import pickle\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import collections\n",
    "import h5py\n",
    "import gc\n",
    "from sortedcontainers import SortedDict, SortedList\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d_pet_pictures.txt', 'rb') as f:\n",
    "    data = f.readlines()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create Greedy Algorithm Helper Functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tagdict(data):\n",
    "    hdict, vdict = {}, {}\n",
    "    for i in range(len(data)):\n",
    "        row = np.vectorize(lambda s: s.decode('utf-8'))(data[i].split())\n",
    "        if row[0]==\"H\": hdict[str(i)] = set(row[1:])\n",
    "        else: vdict[str(i)] = set(row[1:])\n",
    "    return hdict, vdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES INVARIANT\n",
    "#Needs hdict and vdict, dictionaries of image tag sets\n",
    "def calculate_interest(name1, name2):\n",
    "    #Invariant: names are of the form (H/V):K where H/V indicates the dictionary in question and K is the key\n",
    "    #The hdict and vdict dictionaries contain sets\n",
    "    namedisamb = (lambda s: hdict[s.split(\":\")[1]] if s.split(\":\")[0]==\"H\" else \n",
    "                  vdict[s.split(\":\")[1]])\n",
    "    tag1, tag2 = namedisamb(name1), namedisamb(name2)\n",
    "    return min(len(tag1&tag2), len(tag1-tag2), len(tag2-tag1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate interest master list for subset of arrays\n",
    "#Memory limitations prohibit a 90000x90000 master matrix\n",
    "def generate_interest_dict(keys):\n",
    "    start = time.time()\n",
    "    checkpoints = np.arange(0, keys.shape[0], keys.shape[0]/5)[1:]\n",
    "    interestdict = SortedDict()\n",
    "    for i in range(keys.shape[0]):\n",
    "        for j in range(i+1, keys.shape[0]):\n",
    "            value = calculate_interest(keys[i], keys[j])\n",
    "            if value==0: continue\n",
    "            if interestdict.get(value, False)==False:\n",
    "                interestdict[value] = collections.deque([(i,j)])\n",
    "            else:\n",
    "                interestdict[value].append((i,j))\n",
    "        if (i+1) in checkpoints: \n",
    "            print(\"Calculation Checkpoint of \"+str(i+1)+\":\"+str(time.time()-start))\n",
    "    print(time.time()-start)\n",
    "    return interestdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS:\n",
    "#interestdict: a sorted dictionaries of deques containing interest values\n",
    "#lookupdict: a dictionary of occurences\n",
    "#keys: list of names of keys\n",
    "def getmaxfn(interestdict, lookupdict, keys):\n",
    "    curcount1, curcount2 = 2, 2\n",
    "    while (curcount1==2) or (curcount2==2):\n",
    "        if len(interestdict)==0: return None, False\n",
    "        curval = interestdict.keys()[-1]\n",
    "        curmax = interestdict[curval].pop()\n",
    "        curcount1, curcount2 = lookupdict.get(keys[curmax[0]],0), lookupdict.get(keys[curmax[1]],0)\n",
    "        if not interestdict[curval]:\n",
    "            del interestdict[curval]\n",
    "    lookupdict[keys[curmax[0]]] = curcount1+1\n",
    "    lookupdict[keys[curmax[1]]] = curcount2+1\n",
    "    return (keys[curmax[0]], keys[curmax[1]]), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collapse Deque\n",
    "#PARAMETERS:\n",
    "#pair: size-2 tuple containing the new added transition\n",
    "#curarrang: a deque of deques containing current valid transition sequences\n",
    "#lookupdict: a dictionary recording occurences\n",
    "def deque_update(pair, curarrang, lookupdict):\n",
    "    #Resolve Merge of Two Elements\n",
    "    def resolve_double(first, second):\n",
    "        #INVARIANT: The first and second will have an overlapping value as the first was updated but not second\n",
    "        if curarrang[first][0] in [curarrang[second][0], curarrang[second][-1]]:\n",
    "            curarrang[first].reverse()\n",
    "        if curarrang[first][-1]==curarrang[second][0]:\n",
    "            curarrang[second].popleft()\n",
    "        elif curarrang[first][-1]==curarrang[second][-1]:\n",
    "            curarrang[second].reverse()\n",
    "            curarrang[second].popleft()\n",
    "        curarrang[first]+=curarrang[second]\n",
    "        del curarrang[second]\n",
    "    match, lastm = False, 0\n",
    "    i=0\n",
    "    while i<len(curarrang):\n",
    "        if curarrang[i][0]==pair[0]:\n",
    "            #Prevent circular deques\n",
    "            if curarrang[i][-1]==pair[1]: \n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].appendleft(pair[1])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        elif curarrang[i][0]==pair[1]:\n",
    "            if curarrang[i][-1]==pair[0]: \n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].appendleft(pair[0])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        elif curarrang[i][-1]==pair[0]:\n",
    "            if curarrang[i][0]==pair[1]: \n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].append(pair[1])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        elif curarrang[i][-1]==pair[1]:\n",
    "            if curarrang[i][-1]==pair[0]:\n",
    "                lookupdict[pair[0]]-=1\n",
    "                lookupdict[pair[1]]-=1\n",
    "                return\n",
    "            if not match:\n",
    "                curarrang[i].append(pair[0])\n",
    "                lastm =  i\n",
    "                match = True\n",
    "                i+=1\n",
    "            else:\n",
    "                resolve_double(lastm, i)\n",
    "        else:\n",
    "            i+=1\n",
    "    if match: return\n",
    "    curarrang.append(collections.deque(pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Run Single-Image Greedy Heuristic (Layer 1)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "nsplits = 4\n",
    "hdict, vdict = generate_tagdict(data)\n",
    "hkeys = np.vectorize(lambda s: \"H:\"+str(s))(np.array(list(hdict.keys())))\n",
    "vkeys = np.vectorize(lambda s: \"V:\"+str(s))(np.array(list(vdict.keys())))\n",
    "totalkeys = np.append(hkeys, vkeys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(totalkeys.shape[0])\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(indices)\n",
    "indices = np.split(indices, nsplits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_run(keys):\n",
    "    interestdict = generate_interest_dict(keys)\n",
    "    lookupdict = {}\n",
    "    curarrang = collections.deque()\n",
    "    while (len(curarrang)==0) or (len(curarrang[0])<len(keys)):\n",
    "        maxval, valid = getmaxfn(interestdict, lookupdict, keys)\n",
    "        #In case all positive interest metrics have been exhausted\n",
    "        if not valid: break\n",
    "        deque_update(maxval, curarrang, lookupdict)\n",
    "    finalarrang = collections.deque()\n",
    "    for arrang in curarrang:\n",
    "        finalarrang+=arrang\n",
    "    del curarrang\n",
    "    remimg = set(keys)-set(finalarrang)\n",
    "    for img in remimg:\n",
    "        finalarrang.append(img)\n",
    "    return finalarrang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Checkpoint of 4500:424.9527807235718\n",
      "Calculation Checkpoint of 9000:822.3509747982025\n",
      "Calculation Checkpoint of 13500:1063.8601069450378\n",
      "Calculation Checkpoint of 18000:1207.6297268867493\n",
      "1255.2410807609558\n",
      "Mask Optimized in 2244.443123102188 seconds\n",
      "Calculation Checkpoint of 4500:444.54129004478455\n",
      "Calculation Checkpoint of 9000:794.7886159420013\n",
      "Calculation Checkpoint of 13500:1043.4002268314362\n",
      "Calculation Checkpoint of 18000:1190.953207731247\n",
      "1237.5998117923737\n",
      "Mask Optimized in 2285.985775947571 seconds\n",
      "Calculation Checkpoint of 4500:464.16116309165955\n",
      "Calculation Checkpoint of 9000:815.0599539279938\n",
      "Calculation Checkpoint of 13500:1061.4540979862213\n",
      "Calculation Checkpoint of 18000:1215.1491250991821\n",
      "1261.6071569919586\n",
      "Mask Optimized in 2252.183938741684 seconds\n",
      "Calculation Checkpoint of 4500:421.3487648963928\n",
      "Calculation Checkpoint of 9000:752.308002948761\n",
      "Calculation Checkpoint of 13500:993.1060469150543\n",
      "Calculation Checkpoint of 18000:1132.5027568340302\n",
      "1177.8050417900085\n",
      "Mask Optimized in 2160.84285902977 seconds\n"
     ]
    }
   ],
   "source": [
    "finalresults = collections.deque()\n",
    "for mask in indices:\n",
    "    start = time.time()\n",
    "    arrang = individual_run(totalkeys[mask])\n",
    "    gc.collect()\n",
    "    finalresults.append(arrang)\n",
    "    print(\"Mask Optimized in \"+str(time.time()-start)+\" seconds\")\n",
    "pickle.dump(finalresults, open(\"./layer1arrang.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generate Helper Functions for Replacement of Double-Vertical Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INVARIANT: Every element of arrang is a 1-element or 2-element tuple\n",
    "def interest_change(index1, index2, arrang):\n",
    "    def important_index(index, arrang):\n",
    "        if (index==0): vals = [arrang[1]]\n",
    "        elif (index==(len(arrang)-1)): vals = [arrang[index-1]]\n",
    "        else: vals = [arrang[index-1],arrang[index+1]]\n",
    "        return vals\n",
    "    def total_value(images, tup):\n",
    "        nameret = (lambda s: hdict[s.split(\":\")[1]] if s.split(\":\")[0]==\"H\" else \n",
    "                  vdict[s.split(\":\")[1]])\n",
    "        interest = 0\n",
    "        main = nameret(tup[0]) if len(tup)==1 else nameret(tup[0]).union(nameret(tup[1]))\n",
    "        for pair in images:\n",
    "            curset = nameret(pair[0]) if len(pair)==1 else nameret(pair[0]).union(nameret(pair[1]))\n",
    "            interest+=min(len(curset&main), len(curset-main), len(main-curset))\n",
    "        return interest\n",
    "    desvals, sourcevals = important_index(index1, arrang), important_index(index2, arrang)\n",
    "    if index1==index2-1: sourcevals = sourcevals[1:]\n",
    "    elif index1==index2+1: sourcevals = sourcevals[:-1]\n",
    "    preinterest=total_value(desvals, arrang[index1]) + total_value(sourcevals, arrang[index2])\n",
    "    if index1==index2-1:\n",
    "        desvals=desvals[:-1]+sourcevals\n",
    "        sourcevals = []\n",
    "    elif index1==index2+1:\n",
    "        desvals=desvals[1:]+sourcevals\n",
    "        sourcevals = []\n",
    "    postinterest=total_value(desvals, (arrang[index1][0], arrang[index2][0]))\n",
    "    if len(sourcevals)>1:\n",
    "        postinterest+=total_value(sourcevals[1:], sourcevals[0])\n",
    "    return postinterest-preinterest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_swap(vertindices, mainindex, arrang):\n",
    "    bestindex, bestinterest, firstdes = -1, None, True\n",
    "    for index in vertindices:\n",
    "        if index==mainindex: continue\n",
    "        value1 = interest_change(mainindex, index, arrang)\n",
    "        value2 = interest_change(index, mainindex, arrang)\n",
    "        if (not bestinterest) or (value1>bestinterest):\n",
    "            bestindex = index\n",
    "            bestinterest = value1\n",
    "            firstdes = True\n",
    "        if (not bestinterest) or (value2>bestinterest):\n",
    "            bestindex = index\n",
    "            bestinterest = value2\n",
    "            firstdes = False\n",
    "    return bestindex, bestinterest, firstdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Group Double-Vertical Images to Enhance Interest (Layer 2) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresults = pickle.load(open(\"./layer1arrang.pkl\", \"rb\"))\n",
    "vertindices = {i:np.where(np.vectorize(lambda s: 'V' in s)(np.array(finalresults[i])))[0]\n",
    "               for i in range(len(finalresults))}\n",
    "#Uniform data structure (to tuple) for efficient functions\n",
    "for arr in finalresults:\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = (arr[i],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad Tolerance is maximum loss on grouping you are willing to endure. Greater losses can potentially be mitigated\n",
    "#On full 4-level grouping later\n",
    "def grouping_run(arrang, indexset, BAD_TOLERANCE=-4):\n",
    "    i=0\n",
    "    while i<len(arrang):\n",
    "        if len(indexset)<2: return\n",
    "        if (len(arrang[i])==1) and (\"V\" in arrang[i][0]):\n",
    "            bestindex, bestinterest, firstdes = best_swap(indexset, i, arrang)\n",
    "            if not (bestinterest<BAD_TOLERANCE):\n",
    "                indexset = indexset[(indexset!=i)&(indexset!=bestindex)]\n",
    "                if firstdes:\n",
    "                    arrang[i] = (arrang[i][0], arrang[bestindex][0])\n",
    "                    del arrang[bestindex]\n",
    "                    if bestindex>i: i+=1\n",
    "                    indexset[indexset>bestindex]-=1\n",
    "                else:\n",
    "                    arrang[bestindex] = (arrang[i][0], arrang[bestindex][0])\n",
    "                    del arrang[i]\n",
    "                    indexset[indexset>i]-=1\n",
    "            else:\n",
    "                indexset = indexset[(indexset!=i)] #Needs to wait later grouping\n",
    "                i+=1\n",
    "        else:\n",
    "            i+=1\n",
    "        if (i%100==0): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical Combinations Optimized for Mask in 3932.538013935089 seconds\n",
      "Vertical Combinations Optimized for Mask in 4115.945489883423 seconds\n",
      "Vertical Combinations Optimized for Mask in 4273.9554278850555 seconds\n",
      "Vertical Combinations Optimized for Mask in 4310.372158765793 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(nsplits):\n",
    "    start = time.time()\n",
    "    grouping_run(finalresults[i], vertindices[i])\n",
    "    gc.collect()\n",
    "    print(\"Vertical Combinations Optimized for Mask in \"+str(time.time()-start)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#While we used a faster one-element interest calculation in Layer 1 to optimize speed, we need tuple support here\n",
    "def interest(tuple1, tuple2):\n",
    "    nameret = (lambda s: hdict[s.split(\":\")[1]] if s.split(\":\")[0]==\"H\" else \n",
    "                  vdict[s.split(\":\")[1]])\n",
    "    tupleset = (lambda s: nameret(s[0]) if len(s)==1 else nameret(s[0]).union(nameret(s[1])))\n",
    "    set1, set2 = tupleset(tuple1), tupleset(tuple2)\n",
    "    return min(len(set1&set2), len(set1-set2), len(set2-set1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Optimize ordering of the four sub-sequences\n",
    "orders = [p for p in permutations(np.arange(nsplits))]\n",
    "maxgain, maxorder = None, None\n",
    "for order in orders:\n",
    "    gain = 0\n",
    "    for pos in range(1, len(order)):\n",
    "        gain+=interest(finalresults[order[pos-1]][-1], finalresults[order[pos]][0])\n",
    "    if (not maxgain) or (gain>maxgain):\n",
    "        maxgain = gain\n",
    "        maxorder = order\n",
    "arrang = collections.deque()\n",
    "for pos in maxorder:\n",
    "    arrang+=finalresults[pos]\n",
    "del finalresults\n",
    "pickle.dump(arrang, open(\"./layer2arrang.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrang = pickle.load(open(\"./layer2arrang.pkl\", \"rb\"))\n",
    "finalindexset = np.where(np.vectorize(lambda s: (len(s)==1) and (s[0][0]==\"V\"))(arrang))[0]\n",
    "#%time grouping_run(arrang, finalindexset, BAD_TOLERANCE=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finalize and Write Submission File </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Checks on Status of Arrangement\n",
    "#CHECK 1: All Vertical Images Occur in Pairs and Horizontals Occur Individually\n",
    "#CHECK 2: All Images are Unique (i.e. no repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of Final Interest Metric of Produced Output\n",
    "val = 0\n",
    "for i in range(len(arrang)-1):\n",
    "    val+=interest(arrang[i], arrang[i+1])\n",
    "print(\"Final Interest Metric Across Sequence: \"+str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Output File in Desired Format\n",
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    finallength = str(len(arrang))+\"\\n\"\n",
    "    f.write(finallength)\n",
    "    for slide in arrang:\n",
    "        content = \" \".join([s.split(\":\")[1] for s in slide])+\"\\n\"\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
